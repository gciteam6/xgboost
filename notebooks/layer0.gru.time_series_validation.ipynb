{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRUで発電量予測\n",
    "\n",
    "#### やったこと\n",
    "+ GRU(Gated Recurrent Unit)で発電量予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path, pardir\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT_DIRPATH = path.join(os.getcwd(), pardir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(PROJECT_ROOT_DIRPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KWARGS_READ_CSV = {\n",
    "    \"sep\": \"\\t\",\n",
    "    \"header\": 0,\n",
    "    \"parse_dates\": [0],\n",
    "    \"index_col\": 0\n",
    "}\n",
    "KWARGS_RESAMPLING = {\n",
    "    \"rule\": \"30T\",\n",
    "    \"axis\": 0,\n",
    "    \"closed\": \"right\",\n",
    "    \"label\": \"right\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.layers import (\n",
    "    BatchNormalization,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Input,\n",
    "    GRU,\n",
    "    Masking,\n",
    "    TimeDistributed\n",
    ")\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "validation_split=0.2\n",
    "\n",
    "gru_params = {\n",
    "    \"units\": 48,\n",
    "    \"batch_size\": 256,\n",
    "    \"activation\": 'sigmoid',\n",
    "    \"recurrent_activation\": 'sigmoid',\n",
    "    # \"kernel_regularizer\": {\"l1\": 0.01, \"l2\": 0.005},\n",
    "    \"dropout\": 0.1,\n",
    "    \"recurrent_dropout\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_minute = 10\n",
    "validation_year = 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCATIONS = (\n",
    "    \"ukishima\",\n",
    "    \"ougishima\",\n",
    "    \"yonekurayama\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEST_FILEPATH_PREFIX = path.join(PROJECT_ROOT_DIRPATH, \"data\", \"processed\", \"dataset\")\n",
    "TRAIN_TEST_FILEPATH_EXTENTION = \"tsv\"\n",
    "\n",
    "interim_serialize_filename_prefix = path.join(\n",
    "    PROJECT_ROOT_DIRPATH, \"models\", \"gru.interim\", \"fit_model.layer0\"\n",
    ")\n",
    "model_serialize_filename_prefix = path.join(\n",
    "    PROJECT_ROOT_DIRPATH, \"models\", \"gru\", \"fit_model.layer0\"\n",
    ")\n",
    "predict_serialize_filename_prefix = path.join(\n",
    "    PROJECT_ROOT_DIRPATH, \"models\", \"gru\", \"predict.layer0\"\n",
    ")\n",
    "os.makedirs(path.dirname(interim_serialize_filename_prefix), exist_ok=True)\n",
    "os.makedirs(path.dirname(model_serialize_filename_prefix), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(target,\n",
    "                location,\n",
    "                prefix=TRAIN_TEST_FILEPATH_PREFIX,\n",
    "                suffix=TRAIN_TEST_FILEPATH_EXTENTION):\n",
    "    if target == \"train\":\n",
    "        filepath = '.'.join([prefix, \"train_X_y.every_10\", location, suffix])\n",
    "        df = pd.read_csv(filepath, **KWARGS_READ_CSV)\n",
    "\n",
    "        return df.iloc[:, :-1], df.iloc[:, -1]\n",
    "    elif target ==\"test\":\n",
    "        filepath = '.'.join([prefix, \"test_X.every_10\", location, suffix])\n",
    "\n",
    "        return pd.read_csv(filepath, **KWARGS_READ_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_model_name_from_param_dict(param_dict):\n",
    "    model_name = str()\n",
    "    for k, v in sorted(gru_params.items()):\n",
    "        if isinstance(v, dict):\n",
    "            for k_inner, v_inner in v.items():\n",
    "                model_name += \"{ko}_{ki}_{v}.\".format(ko=k, ki=k_inner, v=v_inner)\n",
    "        else:\n",
    "            model_name += \"{k}_{v}.\".format(k=k, v=v)\n",
    "\n",
    "    return model_name[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_modified_param_dict(gru_params):\n",
    "    modified_param_dict = dict()\n",
    "    pt_regularizer = re.compile(\"regularizer\")\n",
    "    for k, v in gru_params.items():\n",
    "        if pt_regularizer.search(k):\n",
    "            modified_param_dict[k] = l1_l2(l1=v['l1'], l2=v['l2'])\n",
    "        else:\n",
    "            modified_param_dict[k] = v\n",
    "\n",
    "    return modified_param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sequential(input_shape, gru_params):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Masking(mask_value=1.)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = GRU(return_sequences=True, **gru_params)(x)\n",
    "    predictions = TimeDistributed(Dense(1, activation=\"linear\"))(x)\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "    optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=optimizer, metrics=[\"mae\", ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_callbacks(target, location):\n",
    "    serialize_path = '.'.join([interim_serialize_filename_prefix,\n",
    "                               \"ep_{epoch:04d}.mae_{mean_absolute_error:.2f}.val_mae_{val_mean_absolute_error:.2f}\",\n",
    "                               \"{t}.{l}.hdf5\".format(t=target, l=location)])\n",
    "\n",
    "    cp_best = ModelCheckpoint(filepath=serialize_path,\n",
    "                              monitor='val_mean_absolute_error',\n",
    "                              save_best_only=True,\n",
    "                              mode='auto')\n",
    "    cp_period = ModelCheckpoint(filepath=serialize_path, period=100)\n",
    "    es = EarlyStopping(monitor='val_mean_absolute_error', min_delta=0, patience=100, mode='auto')\n",
    "\n",
    "    return [cp_best, cp_period, es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_from_index(ndarray, begin_val_index, end_val_index):\n",
    "    if ndarray.ndim == 2:\n",
    "        return ndarray[:begin_val_index, :], ndarray[begin_val_index:end_val_index, :]\n",
    "    elif ndarray.ndim == 3:\n",
    "        return ndarray[:begin_val_index, :, :], ndarray[begin_val_index:end_val_index, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, copy=True):\n",
    "        self.copy = copy\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 浮島"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定数の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = \"ukishima\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### トレーニングデータ取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_x, df_y = get_dataset(\"train\", location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_series = 144\n",
    "num_features = df_x.shape[1]\n",
    "datetime_index = df_y.index.copy(deep=True)\n",
    "is_null_y = df_y.isnull().copy(deep=True)\n",
    "\n",
    "model_name = gen_model_name_from_param_dict(gru_params)\n",
    "modified_param_dict = gen_modified_param_dict(gru_params)\n",
    "modified_param_dict[\"name\"] = model_name\n",
    "target_val = \"foldout_{y}\".format(y=validation_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 系列データ全体でmin-maxスケーリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndarray_x = df_x.as_matrix()\n",
    "ndarray_y = df_y.as_matrix().reshape(-1, 1)\n",
    "is_nan_y_train = np.isnan(ndarray_y.reshape(-1))\n",
    "\n",
    "ndarray_x[is_nan_y_train, :] = 1.\n",
    "ndarray_y[is_nan_y_train, :] = 1.\n",
    "\n",
    "x_scaler = MinMaxScaler()\n",
    "ndarray_x = x_scaler.fit_transform(ndarray_x)\n",
    "\n",
    "y_scaler = DummyScaler()\n",
    "ndarray_y = y_scaler.fit_transform(ndarray_y)\n",
    "\n",
    "ndarray_x[is_nan_y_train, :] = 1.\n",
    "ndarray_y[is_nan_y_train, :] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### データセットの分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndarray_x = ndarray_x.reshape(-1, len_series, num_features).astype('float32')\n",
    "ndarray_y = ndarray_y.reshape(-1, len_series, 1)\n",
    "\n",
    "begin_val_datetime = pd.to_datetime(\"{y}-01-01 00:10:00\".format(y=validation_year))\n",
    "end_val_datetime = pd.to_datetime(\"{y}-01-01 00:10:00\".format(y=validation_year+1))\n",
    "begin_val_index = int(datetime_index.get_loc(begin_val_datetime) / len_series)\n",
    "val_end_index = int(datetime_index.get_loc(end_val_datetime) / len_series)\n",
    "\n",
    "x_train, x_val = split_from_index(ndarray_x, begin_val_index, val_end_index)\n",
    "y_train, y_val = split_from_index(ndarray_y, begin_val_index, val_end_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 欠損サンプルの除去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample_train = x_train.shape[0]\n",
    "num_sample_val = x_val.shape[0]\n",
    "\n",
    "remove_sample_list = [\n",
    "    i for i in range(0, int(df_y.size/len_series))\n",
    "    if is_null_y.iloc[np.arange(len_series*i, len_series*(i+1))].sum() == len_series\n",
    "]\n",
    "remove_samples_train = [\n",
    "    elem\n",
    "    for elem in remove_sample_list\n",
    "    if elem < num_sample_train\n",
    "]\n",
    "remove_samples_val = [\n",
    "    elem - num_sample_train\n",
    "    for elem in remove_sample_list\n",
    "    if num_sample_train <= elem < num_sample_train + num_sample_val\n",
    "]\n",
    "\n",
    "x_train = np.delete(x_train, remove_samples_train, 0)\n",
    "y_train = np.delete(y_train, remove_samples_train, 0)\n",
    "x_val = np.delete(x_val, remove_samples_val, 0)\n",
    "y_val = np.delete(y_val, remove_samples_val, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### モデルの宣言"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gen_sequential((len_series, num_features), modified_param_dict)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### モデリング for validaton data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=validation_split,\n",
    "                    callbacks=gen_callbacks(target_val, location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### validation dataに対する結果の出力および保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_scaler.inverse_transform(model.predict(x_val).reshape(-1, 1))\n",
    "y_true = y_scaler.inverse_transform(y_val.reshape(-1, 1))\n",
    "\n",
    "pd.DataFrame(\n",
    "    y_pred,\n",
    "    index=pd.date_range(begin_val_datetime,\n",
    "                        end_val_datetime + pd.Timedelta(-freq_minute, unit='m'),\n",
    "                        freq=pd.offsets.Minute(freq_minute)),\n",
    "    columns=[model_name,]\n",
    ").to_csv(\n",
    "    '.'.join([predict_serialize_filename_prefix,\n",
    "              model_name,\n",
    "              target_val,\n",
    "              location,\n",
    "              \"tsv\"]),\n",
    "    sep=\"\\t\"\n",
    ")\n",
    "model.save(\n",
    "    '.'.join([model_serialize_filename_prefix,\n",
    "              model_name,\n",
    "              target_val, \n",
    "              location,\n",
    "              \"hdf5\"])\n",
    ")\n",
    "\n",
    "print(\"MAE convert into every 30 in {y}: \".format(y=validation_year),\n",
    "      3 * mean_absolute_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### モデリング for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=gen_callbacks(\"test\", location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test dataに対する結果の保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = get_dataset(\"test\", location)\n",
    "x_test = x_scaler.transform(df_test.as_matrix())\n",
    "\n",
    "y_pred = y_scaler.inverse_transform(\n",
    "    model.predict(\n",
    "        x_test.reshape(-1, len_series, num_features).astype('float32')\n",
    "    ).reshape(-1, 1)\n",
    ")\n",
    "df_y_pred = pd.DataFrame(y_pred, index=df_test.index, columns=[model_name,])\n",
    "\n",
    "df_y_pred.resample(\n",
    "    **KWARGS_RESAMPLING\n",
    ").sum().to_csv(\n",
    "    '.'.join([predict_serialize_filename_prefix,\n",
    "              model_name,\n",
    "              \"test\",\n",
    "              location,\n",
    "              \"tsv\"]),\n",
    "    sep=\"\\t\"\n",
    ")\n",
    "model.save(\n",
    "    '.'.join([model_serialize_filename_prefix,\n",
    "              model_name,\n",
    "              \"test\",\n",
    "              location,\n",
    "              \"hdf5\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 扇島"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定数の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = \"ougishima\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### トレーニングデータ取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x, df_y = get_dataset(\"train\", location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_series = 144\n",
    "num_features = df_x.shape[1]\n",
    "datetime_index = df_y.index.copy(deep=True)\n",
    "is_null_y = df_y.isnull().copy(deep=True)\n",
    "\n",
    "model_name = gen_model_name_from_param_dict(gru_params)\n",
    "modified_param_dict = gen_modified_param_dict(gru_params)\n",
    "modified_param_dict[\"name\"] = model_name\n",
    "target_val = \"foldout_{y}\".format(y=validation_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 系列データ全体でスケーリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndarray_x = df_x.as_matrix()\n",
    "ndarray_y = df_y.as_matrix().reshape(-1, 1)\n",
    "is_nan_y_train = np.isnan(ndarray_y.reshape(-1))\n",
    "\n",
    "ndarray_x[is_nan_y_train, :] = 1.\n",
    "ndarray_y[is_nan_y_train, :] = 1.\n",
    "\n",
    "x_scaler = MinMaxScaler()\n",
    "ndarray_x = x_scaler.fit_transform(ndarray_x)\n",
    "\n",
    "y_scaler = DummyScaler()\n",
    "ndarray_y = y_scaler.fit_transform(ndarray_y)\n",
    "\n",
    "ndarray_x[is_nan_y_train, :] = 1.\n",
    "ndarray_y[is_nan_y_train, :] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### データセットの分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndarray_x = ndarray_x.reshape(-1, len_series, num_features).astype('float32')\n",
    "ndarray_y = ndarray_y.reshape(-1, len_series, 1)\n",
    "\n",
    "begin_val_datetime = pd.to_datetime(\"{y}-01-01 00:10:00\".format(y=validation_year))\n",
    "end_val_datetime = pd.to_datetime(\"{y}-01-01 00:10:00\".format(y=validation_year+1))\n",
    "begin_val_index = int(datetime_index.get_loc(begin_val_datetime) / len_series)\n",
    "val_end_index = int(datetime_index.get_loc(end_val_datetime) / len_series)\n",
    "\n",
    "x_train, x_val = split_from_index(ndarray_x, begin_val_index, val_end_index)\n",
    "y_train, y_val = split_from_index(ndarray_y, begin_val_index, val_end_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 欠損サンプルの除去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample_train = x_train.shape[0]\n",
    "num_sample_val = x_val.shape[0]\n",
    "\n",
    "remove_sample_list = [\n",
    "    i for i in range(0, int(df_y.size/len_series))\n",
    "    if is_null_y.iloc[np.arange(len_series*i, len_series*(i+1))].sum() == len_series\n",
    "]\n",
    "remove_samples_train = [\n",
    "    elem\n",
    "    for elem in remove_sample_list\n",
    "    if elem < num_sample_train\n",
    "]\n",
    "remove_samples_val = [\n",
    "    elem - num_sample_train\n",
    "    for elem in remove_sample_list\n",
    "    if num_sample_train <= elem < num_sample_train + num_sample_val\n",
    "]\n",
    "\n",
    "x_train = np.delete(x_train, remove_samples_train, 0)\n",
    "y_train = np.delete(y_train, remove_samples_train, 0)\n",
    "x_val = np.delete(x_val, remove_samples_val, 0)\n",
    "y_val = np.delete(y_val, remove_samples_val, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### モデルの宣言"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gen_sequential((len_series, num_features), modified_param_dict)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### モデリング for validaton data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=validation_split,\n",
    "                    callbacks=gen_callbacks(target_val, location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### validation dataに対する結果の出力および保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_scaler.inverse_transform(model.predict(x_val).reshape(-1, 1))\n",
    "y_true = y_scaler.inverse_transform(y_val.reshape(-1, 1))\n",
    "\n",
    "pd.DataFrame(\n",
    "    y_pred,\n",
    "    index=pd.date_range(begin_val_datetime,\n",
    "                        end_val_datetime + pd.Timedelta(-freq_minute, unit='m'),\n",
    "                        freq=pd.offsets.Minute(freq_minute)),\n",
    "    columns=[model_name,]\n",
    ").to_csv(\n",
    "    '.'.join([predict_serialize_filename_prefix,\n",
    "              model_name,\n",
    "              target_val,\n",
    "              location,\n",
    "              \"tsv\"]),\n",
    "    sep=\"\\t\"\n",
    ")\n",
    "model.save(\n",
    "    '.'.join([model_serialize_filename_prefix,\n",
    "              model_name,\n",
    "              target_val, \n",
    "              location,\n",
    "              \"hdf5\"])\n",
    ")\n",
    "\n",
    "print(\"MAE convert into every 30 in {y}: \".format(y=validation_year),\n",
    "      3 * mean_absolute_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### モデリング for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=gen_callbacks(\"test\", location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test dataに対する結果の保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = get_dataset(\"test\", location)\n",
    "x_test = x_scaler.transform(df_test.as_matrix())\n",
    "\n",
    "y_pred = y_scaler.inverse_transform(\n",
    "    model.predict(\n",
    "        x_test.reshape(-1, len_series, num_features).astype('float32')\n",
    "    ).reshape(-1, 1)\n",
    ")\n",
    "df_y_pred = pd.DataFrame(y_pred, index=df_test.index, columns=[model_name,])\n",
    "\n",
    "df_y_pred.resample(\n",
    "    **KWARGS_RESAMPLING\n",
    ").sum().to_csv(\n",
    "    '.'.join([predict_serialize_filename_prefix,\n",
    "              model_name,\n",
    "              \"test\",\n",
    "              location,\n",
    "              \"tsv\"]),\n",
    "    sep=\"\\t\"\n",
    ")\n",
    "model.save(\n",
    "    '.'.join([model_serialize_filename_prefix,\n",
    "              model_name,\n",
    "              \"test\",\n",
    "              location,\n",
    "              \"hdf5\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 米倉山"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定数の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = \"yonekurayama\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### トレーニングデータ取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x, df_y = get_dataset(\"train\", location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_series = 144\n",
    "num_features = df_x.shape[1]\n",
    "datetime_index = df_y.index.copy(deep=True)\n",
    "is_null_y = df_y.isnull().copy(deep=True)\n",
    "\n",
    "model_name = gen_model_name_from_param_dict(gru_params)\n",
    "modified_param_dict = gen_modified_param_dict(gru_params)\n",
    "modified_param_dict[\"name\"] = model_name\n",
    "target_val = \"foldout_{y}\".format(y=validation_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 系列データ全体でmin-maxスケーリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ndarray_x = df_x.as_matrix()\n",
    "ndarray_y = df_y.as_matrix().reshape(-1, 1)\n",
    "is_nan_y_train = np.isnan(ndarray_y.reshape(-1))\n",
    "\n",
    "ndarray_x[is_nan_y_train, :] = 1.\n",
    "ndarray_y[is_nan_y_train, :] = 1.\n",
    "\n",
    "x_scaler = MinMaxScaler()\n",
    "ndarray_x = x_scaler.fit_transform(ndarray_x)\n",
    "\n",
    "y_scaler = DummyScaler()\n",
    "ndarray_y = y_scaler.fit_transform(ndarray_y)\n",
    "\n",
    "ndarray_x[is_nan_y_train, :] = 1.\n",
    "ndarray_y[is_nan_y_train, :] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### データセットの分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndarray_x = ndarray_x.reshape(-1, len_series, num_features).astype('float32')\n",
    "ndarray_y = ndarray_y.reshape(-1, len_series, 1)\n",
    "\n",
    "begin_val_datetime = pd.to_datetime(\"{y}-01-01 00:10:00\".format(y=validation_year))\n",
    "end_val_datetime = pd.to_datetime(\"{y}-01-01 00:10:00\".format(y=validation_year+1))\n",
    "begin_val_index = int(datetime_index.get_loc(begin_val_datetime) / len_series)\n",
    "val_end_index = int(datetime_index.get_loc(end_val_datetime) / len_series)\n",
    "\n",
    "x_train, x_val = split_from_index(ndarray_x, begin_val_index, val_end_index)\n",
    "y_train, y_val = split_from_index(ndarray_y, begin_val_index, val_end_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 欠損サンプルの除去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample_train = x_train.shape[0]\n",
    "num_sample_val = x_val.shape[0]\n",
    "\n",
    "remove_sample_list = [\n",
    "    i for i in range(0, int(df_y.size/len_series))\n",
    "    if is_null_y.iloc[np.arange(len_series*i, len_series*(i+1))].sum() == len_series\n",
    "]\n",
    "remove_samples_train = [\n",
    "    elem\n",
    "    for elem in remove_sample_list\n",
    "    if elem < num_sample_train\n",
    "]\n",
    "remove_samples_val = [\n",
    "    elem - num_sample_train\n",
    "    for elem in remove_sample_list\n",
    "    if num_sample_train <= elem < num_sample_train + num_sample_val\n",
    "]\n",
    "\n",
    "x_train = np.delete(x_train, remove_samples_train, 0)\n",
    "y_train = np.delete(y_train, remove_samples_train, 0)\n",
    "x_val = np.delete(x_val, remove_samples_val, 0)\n",
    "y_val = np.delete(y_val, remove_samples_val, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### モデルの宣言"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gen_sequential((len_series, num_features), modified_param_dict)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### モデリング for validaton data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=validation_split,\n",
    "                    callbacks=gen_callbacks(target_val, location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### validation dataに対する結果の出力および保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_scaler.inverse_transform(model.predict(x_val).reshape(-1, 1))\n",
    "y_true = y_scaler.inverse_transform(y_val.reshape(-1, 1))\n",
    "\n",
    "pd.DataFrame(\n",
    "    y_pred,\n",
    "    index=pd.date_range(begin_val_datetime,\n",
    "                        end_val_datetime + pd.Timedelta(-freq_minute, unit='m'),\n",
    "                        freq=pd.offsets.Minute(freq_minute)),\n",
    "    columns=[model_name,]\n",
    ").to_csv(\n",
    "    '.'.join([predict_serialize_filename_prefix,\n",
    "              model_name,\n",
    "              target_val,\n",
    "              location,\n",
    "              \"tsv\"]),\n",
    "    sep=\"\\t\"\n",
    ")\n",
    "model.save(\n",
    "    '.'.join([model_serialize_filename_prefix,\n",
    "              model_name,\n",
    "              target_val, \n",
    "              location,\n",
    "              \"hdf5\"])\n",
    ")\n",
    "\n",
    "print(\"MAE convert into every 30 in {y}: \".format(y=validation_year),\n",
    "      3 * mean_absolute_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### モデリング for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=gen_callbacks(\"test\", location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test dataに対する結果の保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = get_dataset(\"test\", location)\n",
    "x_test = x_scaler.transform(df_test.as_matrix())\n",
    "\n",
    "y_pred = y_scaler.inverse_transform(\n",
    "    model.predict(\n",
    "        x_test.reshape(-1, len_series, num_features).astype('float32')\n",
    "    ).reshape(-1, 1)\n",
    ")\n",
    "df_y_pred = pd.DataFrame(y_pred, index=df_test.index, columns=[model_name,])\n",
    "\n",
    "df_y_pred.resample(\n",
    "    **KWARGS_RESAMPLING\n",
    ").sum().to_csv(\n",
    "    '.'.join([predict_serialize_filename_prefix,\n",
    "              model_name,\n",
    "              \"test\",\n",
    "              location,\n",
    "              \"tsv\"]),\n",
    "    sep=\"\\t\"\n",
    ")\n",
    "model.save(\n",
    "    '.'.join([model_serialize_filename_prefix,\n",
    "              model_name,\n",
    "              \"test\",\n",
    "              location,\n",
    "              \"hdf5\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = get_dataset(\"test\", location)\n",
    "x_test = df_test.as_matrix().reshape(-1, len_series, num_features).astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
